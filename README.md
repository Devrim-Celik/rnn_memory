# memory-matrices
A tensorflow implementation of experiments and models used in 

"Using Fast Weights to Attend to the Recent Past" by Ba et al

"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units" by Le, Jaitly, Hinton 

# 1. Overall Structure
This code allows you to reproduce two different experiments on five different kinds of recurrent models. 

## 1.1 Tasks

The first task is a so-called associative retrieval task, which tests a model's ability to quickly memorize characteristics of a simple sequence. The model is presented with a sequence of unique character-number pairs, followed by a query for a character. It then has to match that character to the corresponding number. For example, the model would be presented with the sequence "c5u3b9i1??u", where "??" marks the beginning of the query. The content of the query is "u", and the correct answer would be "3", since the number that corresponds to "u" in the query was "3".  

The second task is the classification of sequential MNIST-data. For this task, the model is presented with training examples consisting of flattened MNIST-images, i.e. vectors of shape (784,1). The 784 grey-values of the images are presented to the model as a sequence instead of an image, as if the image was scanned row by row, column by column. 

## 1.2 Models

The first and most simple kind of model that can be applied to these tasks is an RNN. It is comparably fast, but struggles with the associative retrieval and only achieves about 50-60% Accuracy on the MNIST-task.

The second model is a standard LSTM as presented by Hochreiter et al. Because of the many different gates, these models have many trainable parameters per recurrent unit, which makes them slow, but they achieve higher accuracy rates than RNNs.

The third possible model is an IRNN, which basically is an RNN that was initialized using the identity matrix for the weight matrix instead of random values. See the paper by Le et al.

The fourth model is the fast-weights architecture as presented by Ba et al, which takes longer per step but achieves better results at an earlier stage of the training. 

The fifth model is the autoconceptor as presented by Jaeger.

# 2. Running the experiments

To run the experiments, generate the necessary data as described below and run the file exp_estimator.py with the necessary flags.

## 2.1 Generating Data

The data is generated by running export_data.py with the flags dataset and path. Path is the (absolute) path to the folder in which you want the data to be created (multiple tfrecords-files will be created, for training and testing), while dataset defines which data to create. You can choose between mnist and associative_retrieval. For example, run `python3 export_data.py --path=path/to/datafolder --dataset=mnist` to create MNIST-data and `python3 export_data.py --path=path/to/datafolder --dataset=associative-retrieval` to create AR-data.

## 2.2 Training the model

To train a model, run "exp_estimator.py" while specifying where to look for the data, where to store checkpoints and summaries and which model to train on which data. For example, `python3 exp_estimator.py --data_path=path/to/datafolder --save_path=path/to/outputfolder --model=autoconceptor --task=associative_retrieval --config=default_ar` would initiate the training of an autoconceptor-cell on the associative retrieval task. You could then follow the training progress using TensorBoard, (--logdir=path/to/outputfolder). The training session is a monitored training session, so the training process can be stopped and restarted at any time.